model:
  backbone:
    name: 'resnet101'  # Deeper backbone for better feature extraction
    pretrained: true
    freeze_stages: 1
    norm_eval: true
    out_indices: [0, 1, 2, 3]
  
  # Enable BiFPN and attention mechanisms
  use_bifpn: true
  use_attention: true 
  attention_type: 'cbam'  # Options: 'self', 'cbam'
  
  fpn:
    in_channels: [256, 512, 1024, 2048]
    out_channels: 256
    num_blocks: 3  # Number of BiFPN blocks
    attention_type: 'cbam'  # Attention type for BiFPN
    extra_convs_on_inputs: true
  
  rpn:
    anchor_scales: [4, 8, 16, 32, 64]  # Added more scales for better coverage
    anchor_ratios: [0.5, 0.75, 1.0, 1.5, 2.0]  # Added more ratios for varied shapes
    anchor_strides: [4, 8, 16, 32, 64]
    target_means: [0.0, 0.0, 0.0, 0.0]
    target_stds: [1.0, 1.0, 1.0, 1.0]
    feat_channels: 256
    use_sigmoid_cls: true
    nms_pre: 2000
    nms_post: 2000
    nms_thr: 0.7
    min_bbox_size: 0
    num_max_proposals: 2000
  
  roi:
    roi_layer:
      type: 'RoIAlign'
      out_size: [7, 7]
      sample_num: 2
    roi_size: [7, 7]
    roi_sample_num: 2
    target_means: [0.0, 0.0, 0.0, 0.0]
    target_stds: [0.1, 0.1, 0.2, 0.2]
    reg_class_agnostic: false
    classes: 5  # background + 4 classes
  
  cascade:
    num_stages: 3
    stage_loss_weights: [1, 0.5, 0.25]
    bbox_reg_weights: 
      - [10.0, 10.0, 5.0, 5.0]
      - [20.0, 20.0, 10.0, 10.0]
      - [30.0, 30.0, 15.0, 15.0]
    iou_thresholds: [0.5, 0.6, 0.7]
  
  mask:
    roi_size: [28, 28]  # Larger ROI size for more detailed masks
    in_channels: 256
    conv_kernel_size: 1
    classes: 5  # background + 4 classes
  
  num_classes: 5  # background + 4 classes
  pretrained: null

# Training configuration
training:
  seed: 42
  epochs: 24  # More epochs for better convergence
  batch_size: 4
  workers: 8
  
  optimizer:
    type: 'AdamW'  # Better optimizer
    lr: 0.0001
    weight_decay: 0.0001
  
  lr_scheduler:
    type: 'cosine'
    T_max: 24
    eta_min: 0.00001
  
  # Class weights - Balance training to focus more on hard-to-detect classes
  class_weights:
    background: 1.0
    Normal: 1.0
    Sclerotic: 2.0         # Higher weight for challenging class
    Partially_sclerotic: 1.5  # Medium weight for partially challenging class
    Uncertain: 1.0
  
  # Loss component weights
  rpn_cls_loss_weight: 1.0
  rpn_bbox_loss_weight: 1.0
  rcnn_cls_weight: 1.2      # Increased weight for classification
  rcnn_bbox_weight: 1.0
  mask_loss_weight: 1.5     # Increased weight for masks
  dice_weight: 0.6          # More weight for Dice loss vs BCE for masks
  
  data:
    dataset_type: 'GlomeruliDataset'
    train_path: 'data/train'
    val_path: 'data/val'
    test_path: 'data/test'
    img_size: [1024, 1024]
    classes: ['GN', 'GL', 'GS']
    
    # Enhanced data augmentations
    use_augmentation: true
    augmentations:
      horizontal_flip:
        p: 0.5
      vertical_flip:
        p: 0.5
      random_rotate_90:
        p: 0.5
      transpose:
        p: 0.5
      random_crop:
        p: 0.3
        height: 800
        width: 800
      random_brightness_contrast:
        p: 0.3
        brightness_limit: 0.2
        contrast_limit: 0.2
      grid_distortion:
        p: 0.3
        num_steps: 5
      elastic_transform:
        p: 0.5
        alpha: 120
        sigma: 120
        alpha_affine: 120
      cutout:
        p: 0.3
        num_holes: 8
        max_h_size: 64
        max_w_size: 64
      hue_saturation_value:
        p: 0.4
        hue_shift_limit: 20
        sat_shift_limit: 30
        val_shift_limit: 20
      blur:
        p: 0.2
        blur_limit: 3
    
    # Normalization
    mean: [0.485, 0.456, 0.406]
    std: [0.229, 0.224, 0.225]
  
  # Checkpointing and logging
  checkpoint_dir: 'experiments/checkpoints/improved_v3'
  log_dir: 'experiments/logs/improved_v3'
  save_freq: 1
  eval_freq: 1
  log_freq: 10
  
  # Distributed training
  distributed: true
  gpu_ids: [0, 1]
  
  # Debugging
  debug: false
  debug_samples: 10